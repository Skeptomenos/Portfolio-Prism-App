# Code Review Findings: Detailed Fixes & Snippets
**Generated:** 2025-12-28 | **Focus:** Actionable code changes for each issue

---

## ðŸ”´ CRITICAL ISSUE #1: Logging Anti-Pattern

### Affected Files Summary
```
ishares.py      5 prints  â†’ Lines 83, 84, 87, 90, 100
vaneck.py       4 prints  â†’ Lines 90, 91, 92, 93, 98, 102
vanguard.py     4 prints  â†’ Lines 554, 555, 556, 558
amundi.py       3 prints  â†’ Lines 215, 222, 223, 225
xtrackers.py    3 prints  â†’ Lines 125, 126, 132
hive_client.py  8 prints  â†’ Lines 170, 205, 219, 395, 428, 447, 450, 453, 907, 912
tr_daemon.py    2 prints  â†’ Lines 262, 280, 282
position_keeper.py 2 prints â†’ Lines 68, 69, 78, 79
pdf_parser.py  10 prints  â†’ Lines 306, 309, 314, 333, 336, 367, 372, 379, 417
validation.py   1 print   â†’ Line 112
echo_bridge.py  2 prints  â†’ Lines 274, 275
stdin_loop.py   3 prints  â†’ Lines 52, 79, 96, 104
metrics.py      1 print   â†’ Line 81
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL: 48 print() statements across 12 files
```

### Fix Pattern Template

#### Pattern 1: Simple Output â†’ logger.info()
**File:** vaneck.py:90-93
```python
# BEFORE
print("\n--- Standalone test successful ---")
print(df.head())
print(f"\nTotal rows: {len(df)}")
print(f"Total weight: {df['weight_percentage'].sum():.2f}%")

# AFTER
logger.info("Standalone test successful")
logger.debug(f"DataFrame head:\n{df.head().to_string()}")
logger.info(f"Total rows: {len(df)}, Total weight: {df['weight_percentage'].sum():.2f}%")
```

#### Pattern 2: Error Handling â†’ logger.error(..., exc_info=True)
**File:** hive_client.py:170
```python
# BEFORE
except Exception as e:
    print(f"Failed to create Supabase client: {e}")
    return None

# AFTER
except Exception as e:
    logger.error(f"Failed to create Supabase client: {e}", exc_info=True)
    return None

# OR (shorter version)
except Exception:
    logger.exception("Failed to create Supabase client")
    return None
```

#### Pattern 3: Interactive Input â†’ Conditional + Logger
**File:** ishares.py:79-100
```python
# BEFORE (breaks in non-interactive mode)
def _prompt_for_product_id(self, isin: str) -> str:
    print(f"\nâš ï¸  Missing Product ID for iShares ETF: {isin}")
    print("   Please visit the iShares website...")
    while True:
        product_id = input(f"   Enter Product ID for {isin} (or 's' to skip): ")
        # ...

# AFTER (safe for headless)
def _prompt_for_product_id(self, isin: str) -> Optional[str]:
    if not sys.stdout.isatty():
        logger.warning(f"Cannot prompt for Product ID in non-interactive mode: {isin}")
        return None
    
    logger.info(f"Missing Product ID for iShares ETF: {isin}")
    logger.info("Please visit the iShares website to find the ETF page...")
    
    try:
        product_id = input(f"Enter Product ID for {isin} (or 's' to skip): ").strip()
        if product_id.lower() == 's':
            return None
        if product_id.isdigit():
            return product_id
        logger.warning("Invalid input (non-numeric). Please enter a numeric Product ID.")
    except EOFError:
        logger.error("No input available (non-interactive environment)")
        return None
```

#### Pattern 4: Test/Demo Code â†’ Move to __main__ Block
**File:** vanguard.py:554-558
```python
# BEFORE (in public method)
def fetch_holdings(self, isin: str) -> pd.DataFrame:
    # ... fetch logic ...
    print(f"\n--- Successfully fetched {len(holdings)} holdings ---")
    print(holdings.head(10))

# AFTER (test file instead)
# In vanguard.py:
def fetch_holdings(self, isin: str) -> pd.DataFrame:
    # ... fetch logic (no prints)
    logger.info(f"Fetched {len(holdings)} holdings for {isin}")
    return holdings

# In tests/test_vanguard.py:
def test_vanguard_fetch():
    adapter = VanguardAdapter()
    holdings = adapter.fetch_holdings("IE00BF0CLV98")
    assert len(holdings) > 0
    print(f"âœ“ Fetched {len(holdings)} holdings")
    print(holdings.head(10))
```

#### Pattern 5: IPC Protocol Framing â†’ logger.debug()
**File:** stdin_loop.py:52, 79, 96, 104
```python
# BEFORE (JSON mixed with print)
print(json.dumps(ready_signal))  # â† This breaks JSON parsing if mixed with errors!

# AFTER (protocol-safe)
logger.debug(f"Ready signal: {ready_signal}")
sys.stdout.write(json.dumps(ready_signal) + "\n")  # â† Explicit write
sys.stdout.flush()

# Better: Use a dedicated method
def send_response(data: dict) -> None:
    """Send JSON response on stdout (for Tauri IPC)."""
    sys.stdout.write(json.dumps(data) + "\n")
    sys.stdout.flush()
    logger.debug(f"Sent IPC response: {data.get('method', 'unknown')}")
```

### Verification Commands

```bash
# Find all remaining print() statements
grep -rn "print(" src-tauri/python/portfolio_src/ --include="*.py" \
  | grep -v "def print" \
  | grep -v "# print" \
  | grep -v "__main__"

# Count by file
grep -rn "print(" src-tauri/python/portfolio_src/ --include="*.py" | cut -d: -f1 | sort | uniq -c | sort -rn

# Validate no prints exist in production
python -m py_compile src-tauri/python/portfolio_src/**/*.py && echo "âœ“ Syntax OK"
```

---

## ðŸ”´ CRITICAL ISSUE #2: Adapter Sequential Execution

### Current Bottleneck: Synchronous Network Calls

**File:** `src-tauri/python/portfolio_src/adapters/registry.py:149-180`
```python
def fetch_holdings(self, isin: str, force_refresh: bool = False) -> pd.DataFrame:
    # âŒ PROBLEM: Each adapter blocks until response
    # With 50 ETFs Ã— 3-5s per request = 250-300 second wait
    if self._use_cache:
        return self.holdings_cache.get_holdings(
            isin=isin,
            adapter_registry=self,
            force_refresh=force_refresh,
        )
```

### Solution: Async Wrapper

**File:** Create `src-tauri/python/portfolio_src/adapters/async_registry.py`
```python
"""
Async wrapper for adapter registry.
Enables concurrent ETF holdings fetching without blocking.
"""

import asyncio
import concurrent.futures
import logging
from typing import Dict, Optional
import pandas as pd

from portfolio_src.adapters.registry import AdapterRegistry
from portfolio_src.prism_utils.logging_config import get_logger

logger = get_logger(__name__)

class AsyncAdapterRegistry:
    """Async wrapper around AdapterRegistry for concurrent fetching."""
    
    def __init__(
        self, 
        registry: Optional[AdapterRegistry] = None,
        max_workers: int = 5,  # Respect API rate limits
    ):
        self.registry = registry or AdapterRegistry()
        self.executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix="adapter_fetch"
        )
    
    async def fetch_holdings_batch(
        self,
        isins: list[str],
        force_refresh: bool = False,
    ) -> Dict[str, pd.DataFrame]:
        """
        Fetch holdings for multiple ISINs concurrently.
        
        Args:
            isins: List of ISIN strings
            force_refresh: Skip cache and fetch fresh
        
        Returns:
            Dict mapping ISIN â†’ DataFrame (empty if fetch failed)
        """
        loop = asyncio.get_event_loop()
        
        # Create tasks for each ISIN (runs in thread pool, max 5 concurrent)
        tasks = [
            loop.run_in_executor(
                self.executor,
                self._fetch_safe,
                isin,
                force_refresh
            )
            for isin in isins
        ]
        
        # Wait for all to complete (with exception handling)
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Map results: ISIN â†’ DataFrame or empty if failed
        output = {}
        for isin, result in zip(isins, results):
            if isinstance(result, Exception):
                logger.error(f"Failed to fetch {isin}: {result}")
                output[isin] = pd.DataFrame()
            else:
                output[isin] = result
        
        return output
    
    def _fetch_safe(self, isin: str, force_refresh: bool = False) -> pd.DataFrame:
        """Wrapper for sync fetch_holdings (safe for executor)."""
        try:
            logger.info(f"Fetching holdings for {isin}...")
            result = self.registry.fetch_holdings(isin, force_refresh)
            logger.info(f"âœ“ Fetched {len(result)} holdings for {isin}")
            return result
        except Exception as e:
            logger.error(f"Fetch failed for {isin}: {e}", exc_info=True)
            return pd.DataFrame()
    
    async def fetch_and_decompose(
        self,
        isins: list[str],
        decomposer,
        force_refresh: bool = False,
    ) -> Dict[str, pd.DataFrame]:
        """
        Fetch all holdings concurrently, then decompose.
        
        Args:
            isins: List of ETF ISINs
            decomposer: ETF decomposer instance
            force_refresh: Skip cache
        
        Returns:
            Dict: ISIN â†’ decomposed holdings DataFrame
        """
        # Phase 1: Fetch all holdings (max 5 concurrent)
        holdings_map = await self.fetch_holdings_batch(isins, force_refresh)
        
        logger.info(f"Phase 1 complete: Fetched {len(holdings_map)} ETFs")
        
        # Phase 2: Decompose each (can also be async if decomposer supports it)
        decomposed = {}
        for isin, holdings in holdings_map.items():
            if holdings.empty:
                decomposed[isin] = pd.DataFrame()
            else:
                try:
                    result = await loop.run_in_executor(
                        self.executor,
                        decomposer.decompose,
                        holdings,
                        isin
                    )
                    decomposed[isin] = result
                except Exception as e:
                    logger.error(f"Decomposition failed for {isin}: {e}")
                    decomposed[isin] = pd.DataFrame()
        
        return decomposed
    
    def shutdown(self) -> None:
        """Clean up executor threads."""
        self.executor.shutdown(wait=True)
        logger.info("Async adapter registry shut down")
```

### Integration Point: Pipeline

**File:** Update `src-tauri/python/portfolio_src/core/pipeline.py`
```python
# At top of file, add import
from portfolio_src.adapters.async_registry import AsyncAdapterRegistry

class Pipeline:
    def __init__(self, ...):
        # ... existing init ...
        self._async_registry = AsyncAdapterRegistry(max_workers=5)
    
    async def _decompose_etf_holdings(self, etf_list: list[str]) -> Dict[str, pd.DataFrame]:
        """
        Decompose ETF holdings concurrently.
        
        Previous: 50 ETFs Ã— 5s = 250s
        New: 50 ETFs Ã· 5 parallel = 50s (5x faster!)
        """
        logger.info(f"Starting concurrent decomposition of {len(etf_list)} ETFs...")
        
        # Fetch all concurrently (max 5 in flight)
        holdings_map = await self._async_registry.fetch_holdings_batch(
            etf_list,
            force_refresh=False
        )
        
        logger.info(f"âœ“ Fetch complete. Decomposing {len(holdings_map)} ETFs...")
        
        # Decompose each (can also parallelize if needed)
        decomposed = {}
        for isin, holdings in holdings_map.items():
            if holdings.empty:
                decomposed[isin] = pd.DataFrame()
            else:
                decomposed[isin] = self.decomposer.decompose(holdings, isin)
        
        return decomposed
    
    def run(self, portfolio_id: int = 1, ...):
        # Use async wrapper inside sync context
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            decomposed = loop.run_until_complete(
                self._decompose_etf_holdings(etf_list)
            )
            # Continue with rest of pipeline...
        finally:
            loop.close()
            self._async_registry.shutdown()
```

### Benchmark Target
```
BEFORE (sequential):     50 ETFs Ã— 5s = 250 seconds
AFTER (5 parallel):      50 ETFs Ã· 5 = 50 seconds
IMPROVEMENT:            5x faster (5 minutes vs 4 seconds)
```

---

## ðŸ”´ CRITICAL ISSUE #3: Type Safety - `as any` Casts

### Issue 1: File Type Casting

**File:** `src/components/HoldingsUpload.tsx:55`
```typescript
// BEFORE (unsafe)
const filePath = (file as any).path || file.name;

// AFTER (type-safe)
interface FileWithPath extends File {
  path?: string;
}

function getFilePath(file: File | FileWithPath): string {
  if ('path' in file && typeof (file as any).path === 'string') {
    return (file as FileWithPath).path;
  }
  return file.name;
}

// Usage:
const filePath = getFilePath(file);
```

### Issue 2: Error Type Casting

**File:** `src/components/views/xray/ActionQueue.tsx:72`
```typescript
// BEFORE
{failure.error || (failure as any).issue}

// AFTER (define error shape)
interface FailureResult {
  error?: string;
  issue?: string;
  message?: string;
}

const getErrorMessage = (failure: unknown): string => {
  if (typeof failure === 'object' && failure !== null) {
    const f = failure as FailureResult;
    return f.error || f.issue || f.message || 'Unknown error';
  }
  return String(failure);
};

// Usage:
<div className="item-error">{getErrorMessage(failure)}</div>
```

### Global Fix: Update Types File

**File:** `src/types/index.ts` (add or extend)
```typescript
// File upload types
export interface FileWithPath extends File {
  path?: string;
}

// Error handling types
export interface APIFailure {
  error?: string;
  issue?: string;
  message?: string;
  code?: string;
  statusCode?: number;
}

// Type guards
export function isFileWithPath(file: File | FileWithPath): file is FileWithPath {
  return 'path' in file && typeof (file as any).path === 'string';
}

export function isAPIFailure(value: unknown): value is APIFailure {
  return typeof value === 'object' && value !== null;
}
```

### Validation Script

```bash
# Find all remaining 'as any' casts
grep -rn " as any" src/ --include="*.tsx" --include="*.ts" | grep -v "\.test\." | wc -l

# Should be 0 after fixes
```

---

## ðŸŸ¡ WARNING #1: Database Connection Management

### Current Inconsistency

**File:** `src-tauri/python/portfolio_src/data/database.py`

```python
# âœ… PATTERN A: Correct (sync_positions_from_tr, line 401)
with transaction() as conn:
    conn.execute("INSERT ...")
    # Auto commits on exit

# âŒ PATTERN B: Inconsistent (update_sync_state, line 238)
with get_connection() as conn:
    conn.execute("INSERT ... ON CONFLICT ... DO UPDATE SET ...")
    conn.commit()  # Manual commit (forgettable!)

# âŒ PATTERN C: Direct commit (log_system_event, line 282)
with get_connection() as conn:
    conn.execute("INSERT ...")
    conn.commit()
```

### Unified Fix: Use `transaction()` Everywhere

```python
# WRONG (current pattern A at line 237)
def update_sync_state(source: str, status: str, message: str = "") -> None:
    with get_connection() as conn:
        conn.execute(...)
        conn.commit()  # â† Manual, forgettable

# RIGHT (refactored)
def update_sync_state(source: str, status: str, message: str = "") -> None:
    with transaction() as conn:
        conn.execute(
            """
            INSERT INTO sync_state (source, last_sync, status, message)
            VALUES (?, CURRENT_TIMESTAMP, ?, ?)
            ON CONFLICT(source) DO UPDATE SET
                last_sync = CURRENT_TIMESTAMP,
                status = excluded.status,
                message = excluded.message
        """,
            (source, status, message),
        )
        # conn.commit() happens automatically on context exit
```

### Affected Functions (Refactor Script)

```python
# Apply to these 3 functions:
# 1. update_sync_state() â€” line 237-250
# 2. log_system_event() â€” line 262-282
# 3. mark_logs_processed() â€” line 299-308

# Pattern:
# OLD: with get_connection() as conn: ... conn.commit()
# NEW: with transaction() as conn: ... (auto-commit)
```

---

## ðŸŸ¡ WARNING #2: Error Handling - Missing `exc_info=True`

### Find All Violations

```bash
grep -rn "logger.error(" src-tauri/python/portfolio_src/ --include="*.py" \
  | grep -v "exc_info=True" \
  | head -20
```

### Fix Template

```python
# BEFORE
except Exception as e:
    logger.error(f"Failed to process {isin}: {e}")

# AFTER
except Exception as e:
    logger.error(f"Failed to process {isin}: {e}", exc_info=True)

# OR (shorthand)
except Exception:
    logger.exception(f"Failed to process {isin}")
```

### Critical Files to Audit
- `data/hive_client.py` (8 violations)
- `data/proxy_client.py`
- `data/resolution.py`
- `headless/handlers/sync.py`

---

## ðŸŸ¡ WARNING #3: IPC Type Validation

### Add Zod Validation Layer

**File:** Create `src/lib/ipc-schemas.ts`
```typescript
import { z } from 'zod';

// Define shape of all IPC responses
const XRayHoldingSchema = z.object({
  isin: z.string().optional(),
  stock: z.string(),
  ticker: z.string(),
  totalValue: z.number(),
  resolutionStatus: z.enum(['resolved', 'unresolved', 'skipped']),
  resolutionConfidence: z.number(),
  resolutionSource: z.string().optional(),
});

const ResolutionSummarySchema = z.object({
  total: z.number(),
  resolved: z.number(),
  unresolved: z.number(),
  skipped: z.number(),
  unknown: z.number(),
  bySource: z.record(z.string(), z.number()),
  healthScore: z.number(),
});

export const TrueHoldingsResponseSchema = z.object({
  holdings: z.array(XRayHoldingSchema),
  summary: ResolutionSummarySchema,
});

export type TrueHoldingsResponse = z.infer<typeof TrueHoldingsResponseSchema>;
```

**File:** Update `src/lib/ipc.ts`
```typescript
import { TrueHoldingsResponseSchema, TrueHoldingsResponse } from './ipc-schemas';

export async function getTrueHoldings(): Promise<TrueHoldingsResponse> {
  const raw = await invoke<unknown>('get_true_holdings');
  
  try {
    return TrueHoldingsResponseSchema.parse(raw);
  } catch (error) {
    console.error('[IPC] getTrueHoldings validation failed:', error);
    throw new Error('Invalid response structure from engine');
  }
}
```

---

## ðŸŸ¡ WARNING #4: ISIN Input Validation

### Add Pydantic Validation

**File:** `src-tauri/python/portfolio_src/data/resolution.py` (top of file)
```python
from pydantic import BaseModel, field_validator
import re

class ResolutionRequest(BaseModel):
    """Validated input for ISIN resolution."""
    
    ticker: str
    name: str
    
    @field_validator('ticker')
    @classmethod
    def validate_ticker(cls, v: str) -> str:
        """Validate and normalize ticker."""
        if not v or len(v) > 50:
            raise ValueError("Ticker must be 1-50 characters")
        
        # Allow: uppercase letters, digits, dots, hyphens
        if not re.match(r'^[A-Z0-9\.\-]+$', v.upper()):
            raise ValueError("Ticker contains invalid characters")
        
        return v.upper()
    
    @field_validator('name')
    @classmethod
    def validate_name(cls, v: str) -> str:
        """Validate company name."""
        if not v or len(v) > 200:
            raise ValueError("Name must be 1-200 characters")
        
        # Prevent SQL injection attempts
        if any(char in v for char in [';', '--', '/*', '*/']):
            raise ValueError("Name contains invalid characters")
        
        return v.strip()
```

### Use in Resolution

```python
class ISINResolver:
    def resolve(self, ticker: str, name: str) -> ResolutionResult:
        # Validate inputs first
        try:
            req = ResolutionRequest(ticker=ticker, name=name)
        except ValidationError as e:
            logger.error(f"Invalid resolution request: {e}")
            return ResolutionResult(
                isin=None,
                status="unresolved",
                detail="invalid_input",
                confidence=0.0
            )
        
        # Continue with safe inputs
        return self._resolve_internal(req.ticker, req.name)
```

---

## ðŸŸ¡ WARNING #5: Environment Variable Validation

### Add Startup Check

**File:** `src-tauri/python/portfolio_src/prism_headless.py` (entry point)
```python
import os
from portfolio_src.prism_utils.logging_config import get_logger

logger = get_logger(__name__)

def validate_environment() -> dict[str, bool]:
    """Validate critical environment variables at startup."""
    features = {
        'finnhub': False,
        'hive_sync': False,
    }
    
    # Check Finnhub API key
    finnhub_key = os.getenv('FINNHUB_API_KEY', '').strip()
    if finnhub_key and len(finnhub_key) > 10:
        features['finnhub'] = True
        logger.info("âœ“ Finnhub API configured")
    else:
        logger.warning("Finnhub API key not configured. ISIN resolution will be slower.")
    
    # Check Supabase
    supabase_url = os.getenv('SUPABASE_URL', '').strip()
    if supabase_url:
        features['hive_sync'] = True
        logger.info("âœ“ Supabase configured for Hive sync")
    else:
        logger.warning("Supabase not configured. Community ISIN data unavailable.")
    
    return features

# In main():
if __name__ == '__main__':
    features = validate_environment()
    # Pass features to pipeline/resolver for conditional behavior
```

---

## Summary: Fix Priority Order

**High Impact, Low Effort:**
1. âœ… Print â†’ Logger (2-3 hours, unblocks SN workstream)
2. âœ… Database connection pattern (30 minutes)
3. âœ… Add `exc_info=True` (20 minutes)

**High Impact, Medium Effort:**
4. âœ… Async adapters (4-6 hours, TASK-612)
5. âœ… Type safety fixes (1 hour)

**Medium Impact, Low Effort:**
6. âœ… Add Pydantic validation (45 minutes)
7. âœ… Add Zod validation (1 hour)
8. âœ… Env var validation (30 minutes)
9. âœ… useMemo deps (15 minutes)

**Total: 11-13 hours of focused work**

All changes are backwards-compatible and can be deployed incrementally.
