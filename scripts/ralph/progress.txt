# Ralph Progress Log - Phase 5: Bug Fixes
Started: Sun Jan 11 2026
PRD: prd-phase5.json
---

## 2026-01-11 10:35 - US-001
- Implemented defensive NaN handling in asset_class split
- Files changed:
  - `portfolio_src/core/pipeline.py` - Added `.fillna('').str.upper()` pattern
  - `tests/test_phase5_bugfixes.py` - New test file for Phase 5 bug fixes
- **Learnings for future iterations:**
  - Modern pandas handles None gracefully in `.str.upper()`, but explicit `.fillna('')` is more defensive
  - Pre-existing test failures exist on main (10+ tests) - these are unrelated to Phase 5 changes
  - Patch database functions at `portfolio_src.data.database.get_positions`, not at pipeline module level
---

## 2026-01-11 10:40 - US-002
- Implemented weight format auto-detection in decomposer
- Files changed:
  - `portfolio_src/core/services/decomposer.py` - Added `_normalize_weight_format()` helper function
  - `tests/test_phase5_bugfixes.py` - Added 4 tests for weight format detection
- **Learnings for future iterations:**
  - Weight columns can be named: weight, Weight, weight_pct, Weight_Pct
  - Detection heuristic: if max(weights) <= 1.0 AND sum(weights) <= 2.0, it's decimal format
  - Normalization happens in `_get_holdings()` after fetching, before ISIN resolution
---

## 2026-01-11 10:41 - US-003
- Made Hive contribution asynchronous using daemon thread
- Files changed:
  - `portfolio_src/core/services/decomposer.py` - Added `_contribute_to_hive_async()` helper, replaced sync call
  - `tests/test_phase5_bugfixes.py` - Added 3 tests for async Hive contribution
- **Learnings for future iterations:**
  - Use `threading.Thread(target=fn, daemon=True)` for fire-and-forget operations
  - Daemon threads don't block process exit
  - Mock `get_hive_client` at module level: `portfolio_src.core.services.decomposer.get_hive_client`
  - Time-based tests: use `time.sleep()` in mock side_effect to simulate slow operations
---

## 2026-01-11 10:45 - US-004
- Fixed first-wins aggregation to use highest confidence source
- Files changed:
  - `portfolio_src/core/services/aggregator.py` - Added `_sort_by_confidence()` method, preserved resolution_confidence in _process_direct_positions and _process_etf_positions
  - `tests/test_phase5_bugfixes.py` - Added 4 tests for highest confidence aggregation
- **Learnings for future iterations:**
  - Sort by confidence BEFORE groupby so 'first' picks highest confidence row
  - When confidence is equal, prefer non-'Unknown' values using unknown_penalty sort key
  - Must preserve resolution_confidence column through _process_direct_positions and _process_etf_positions
  - Pre-existing pandas stub type errors are expected and should be ignored
---

## 2026-01-11 10:50 - US-005
- Verified and tested vectorized value calculation
- Files changed:
  - `tests/test_phase5_bugfixes.py` - Added 5 tests for vectorized value calculation
- **Learnings for future iterations:**
  - `calculate_position_values()` in core/utils.py is the single source of truth for position value calculation
  - The function is called ONCE before the loop, and values are looked up by index in the loop
  - Comment on line 832 of pipeline.py explains the vectorized approach: "Calculate ALL values vectorized ONCE (not per row)"
  - The method is `_write_breakdown_report`, not `_write_holdings_breakdown`
---

## 2026-01-11 10:54 - US-006
- Implemented ISIN deduplication before enrichment to avoid redundant API calls
- Files changed:
  - `portfolio_src/core/services/enricher.py` - Refactored `enrich()` to collect unique ISINs first, call enrichment once, then apply to all holdings
  - `tests/test_phase5_bugfixes.py` - Added 4 tests for ISIN deduplication
- **Learnings for future iterations:**
  - Enrichment happens in `Enricher.enrich()`, not in decomposer
  - Use `_collect_unique_isins()` helper to gather all unique ISINs from holdings_map
  - Use `_apply_enrichment_data()` helper to apply pre-fetched data to each DataFrame
  - Log format: "Enrichment: N unique ISINs from M total holdings"
---

## 2026-01-11 10:58 - US-007
- Added division by zero protection in aggregator with explicit if/else clause
- Files changed:
  - `portfolio_src/core/services/aggregator.py` - Replaced ternary with explicit if/else for portfolio_percentage calculation
  - `tests/test_phase5_bugfixes.py` - Added 4 tests for zero total_value scenarios
- **Learnings for future iterations:**
  - The existing ternary expression was functionally correct, but explicit if/else is clearer
  - When total_value <= 0, portfolio_percentage is set to 0.0 (scalar broadcasts to all rows)
  - Pre-existing pandas stub type errors are expected and should be ignored
---

## 2026-01-11 11:02 - US-008
- Fixed inconsistent value calculation logic - ETF values now use canonical helper
- Files changed:
  - `portfolio_src/core/pipeline.py` - Refactored ETF value calculation in `_write_breakdown_report()` to use `calculate_position_values()` instead of manual iterrows()
  - `tests/test_phase5_bugfixes.py` - Added 5 tests for consistent value calculation
- **Learnings for future iterations:**
  - `calculate_position_values()` in core/utils.py is the SINGLE SOURCE OF TRUTH for position value calculation
  - Formula priority: 1) market_value column if exists, 2) quantity * price, 3) zeros with warning
  - Both direct positions AND ETF positions should use this helper for consistency
  - The helper returns a pd.Series indexed same as input DataFrame, use `.get(idx, 0.0)` to lookup values
---

## Codebase Patterns
- Import ValidationGates and related types from `portfolio_src.core.contracts`
- Enricher.enrich() collects unique ISINs first, enriches once, then applies to all holdings
- Pipeline services are initialized lazily in `_init_services()` method
- Instance variables for services use Optional[Type] = None pattern
- Tests run via `python3 -m pytest tests/ -v` from src-tauri/python directory
- Pre-existing type errors exist (pandas stubs) - ignore them
- Use write_json_atomic from core/utils.py as reference for atomic writes
- Telemetry singleton accessed via get_telemetry()
- TDD approach: write failing test FIRST, then implement fix
- ValidationGates.get_pipeline_quality() returns the DataQuality object
- ValidationIssue.actual contains values as strings (e.g., "85.5%")

---
